---
title: "Clustering - Example on the Iris data"
author: "YOUR NAME"
date: "`r Sys.Date()`"
output: html_document
---

# Clustering on the Iris data 

```{r, message=FALSE}
library(tidyverse)
```

## Load and inspect the data
0. First we load the iris data set.
```{r}
data(iris)
```

1. Use `head()` or `View()` to inspect the data. How many features and samples are in the data?
```{r}
# to be filled
```

The data contains 4 numerical measurements (features) for each flower (Sepal.Length Sepal.Width Petal.Length Petal.Width) and a label on which Species the flower is.

## Visualize pair-wise relationships
2. Let's first take a look at individual features in a pairwise manner. Create pairwise scatterplots for each pair of features and color the points by species.
```{r}
# to be filled
```


*Advanced*: Try to use a for-loop or lapply statement to produce all pairwise plots and save them in a list. You can use the `plot_grid` function in the `cowplot` package to nicely arrange all plots.
```{r}
# to be filled (optional)
```

3. We will separate the label on the species and the numerical measurements (contained in the first 4 columns). Create two new objects: one matrix `iris_x` containing the numerical measurements as a matrix and one character vector  `iris_species` containing the species label.
```{r}
# to be filled
```

4. Several of the features seem highly correlated. Calculate the correlation between the 4 features. Which ones are most correlated?
```{r}
# to be filled
```

Side note: A useful function to do the above is `ggpairs` function from the `GGally` package which plots all pairwise relationships. Run the following code to see how this works. If needed, install the GGally package before running this code.
```{r}
library(GGally)
ggpairs(as.data.frame(iris_x), aes(col = iris$Species))
```



## Clustering
Now we would like to cluster the flowers into groups.

### Warm-up exercise
5a. Calculate the distance between the flowers based on the 4 measurements using the `dist` function. 
```{r}
# to be filled
```

5b. What class is the object obtained in 5a?
```{r}
# to be filled
```

5c. Now calculate the average distance of flowers across all species.
```{r}
# to be filled
```

5d. Repeat steps 5a and 5c for each of the three species separately considering only the distances between flowers from the same species. How do the average distances within a species compare to the distances across all species?
```{r}
# to be filled
```

### k-means
Now we want to cluster the flowers into groups. Let's first use kmeans to do this. 

6a. Use the `kmeans` function on the matrix `iris_x` and set the number of cluster as `centers = 3`. Save the output in a variable called `iris_km`. Note: Call `?kmeans` for help on using the function.
```{r}
# to be filled
```

The resulting cluster assignments for each flower can be obtained by extracting the `cluster` entry from the list `iris_km` generated by the `kmeans` function. 

6b. What does `as.factor` do here?
```{r}
iris_clusters_km <- as.factor(iris_km$cluster)
```

Let's compare the clusters we obtained to the labels. 

6c. For this use the `table` command with two arguments: the original species labels `iris_species` and the clusters from our kmeans approach `iris_clusters_km` obtained in the last line of code. 
```{r}
# to be filled
```

6d. How do you interpret the output? (Note: This matrix is also referred to as "confusion matrix").

We can also visualize the cluster labels on our scatterplots. 

6e. Generate a scatterplot of Sepal.Length and Sepal.Width coloured by the identified clusters and by the species label and compare the two. You can also use the `shape` and `color` aesthetics in ggplot to show cluster and species in one plot.
```{r}
# to be filled
```

#### Cluster stability (optional)

7a. Re-run the clustering using the same steps as above (6a-e) with different random seed using the `set.seed()` function before calling the `kmeans` function. What do you find? How does the confusion matrix and the scatterplot change?
```{r}
# to be filled
```

7b. Now repeat the clustering with 10 clusters and for three different random seeds. What do you notice?
```{r}
# to be filled
```

Note on exercise 7b.: You can also set the `nstart` argument in `kmeans` to let the algorithm try different random centers and use the best for the final clustering.

8. (advanced): To determine a good number of clusters we can calculate the within-cluster sum of squares for clustering with different number of clusters. 

8a. Use a for-loop or the `sapply` function to cluster the data into 1 to 12 clusters and for each clustering extract the within-cluster sum of squares from the output, which you can find in the `tot.withinss` component. 
```{r}
# to be filled
```

8b.Generate a Elbow plot visualising the within-cluster sum of squares for each number of clusters.
```{r}
# to be filled
```

### Hierarchical clustering

#### Constructing the dendrogram
9. To use hierarchical clustering we can use the `hclust` function. This function operates on distances objects. 

9a. Use the distance object you calculated in step 5a and call the `hclust` function on it. 
```{r}
# to be filled
```

9b. What class is the output?
```{r}
# to be filled
```

9c. The class has its own plot function. Call the `plot` function on the output of the `hclust` function.
```{r}
# to be filled
```

#### Finding clusters
To generate cluster labels for each flower into 3 clusters we need to cut the tree into three parts. To visualize this, we can plot some boxes that annotate the clusters using the `rect.hclust()` function on the output of the `hclust` function. Let's do this using 3 clusters (`k=3`). Note this required to be directly called together with the `plot` function above because it adds a layer to the existing plot. You can also add the Species names as labels using the `label` argument.

10a. Run the following code and interpret its output.
```{r}
plot(hclust.out, labels = FALSE, main = "Iris dendrogram", )
rect.hclust(hclust.out, k= 3, border = 3:5)
```

To extract the cluster labels for each flower we can use
```{r}
clusterid_hc <- cutree(hclust.out, 3)
```

#### Visualize the cluster labels
We can again visualize the cluster labels on our scatterplots. 

11. Generate a scatterplot of Sepal.Length and Sepal.Width coloured by the identified clusters and by the species label and compare the two. You can also use the `shape` and `color` aesthetics in ggplot to show cluster and species in one plot.
```{r}
# to be filled
```


(Optional) Question: Do you expect different results from hierarchical clustering for different random seeds if you want to cluster the data into 10 cluster? Test this!

Advanced exercise (optional): Can you also generate a clustering of the 4 features (instead of the samples) using hierarchical clustering? Which features are most similar? What happens if you first use `scale` on the iris_x object? You can also change the distance metric and use for example correlation distance, defined as 1 - correlation. Hint: Use `as.dist()` to transform the results of 1- correlation into a distance object that `hclust` can handle. How does this change the results?
```{r}
# to be filled
```

#### Using pheatmap
The clustering of the `pheatmap` function is also based on hierarchical clustering to order the rows and columns. 

12. Call `pheatmap` on the Iris data. You can also test different distance measures by specifying the `clustering_distance_cols` or `clustering_distance_rows`. Note: Don't forget to load the pheatmap library to be able to use the function `pheatmap`.
```{r}
# to be filled
```


```{r}
sessionInfo()
```

