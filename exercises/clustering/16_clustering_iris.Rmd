---
title: "Clustering - Example on the Iris data"
author: "YOUR NAME"
date: "`r Sys.Date()`"
output: html_document
---

# Clustering on the Iris data 

```{r, message=FALSE}
library(tidyverse)
```

## Load and inspect the data
0. First we load the iris data set.
```{r}
data(iris)
```

1. Use `head()` or `View()` to inspect the data. How many features and samples are in the data?
```{r}
# to be filled
```
The data contains 4 numerical measurements (features) for each flower (Sepal.Length Sepal.Width Petal.Length Petal.Width) and a label on which Species the flower is.


2. Let's first take a look at individual features in a pairwise manner. Create pairwise scatterplots for each pair of features and color the points by species.
```{r}
# to be filled
```

*Advanced*: Try to use a for-loop to produce all pairwise plots and save them in a list. You can use the `plot_grid` function in the `cowplot` package to nicely arrange all plots.
```{r}
# to be filled (optional)
```

3. We will separate the label on the species and the numerical measurements (contained in the first 4 columns). Create two new objects: one matrix `iris_x` containing the numerical measurements as a matrix and one character vector  `iris_species` containting the species label.
```{r}
# to be filled
```

4. Several of the features seem highly correlated. Calculate the correlation between the 4 features. Which ones are most correlated?
```{r}
# to be filled
```
Side note: A useful function to do the above is `ggpairs` function from the `GGally` package which plots all pairwise relationships. Run the following code to see how this works. If needed, install the GGally package before running this code.
```{r}
library(GGally)
ggpairs(iris_x, aes(col = iris$Species))
```



## Clustering
Now we would like to cluster the flowers into groups.

### k-means
5. Calculate the distance between the flowers based on the 4 measurements using the `dist` function. Then calculate the average distance of flowers across all species and within each Species. Where do you expect find the smalles/largest average distance? What distance measure is used here?
```{r}
# to be filled
```

Now we want to cluster the flowers into groups. Let's first use kmeans to do this. For this we will use the `kmeans` function on the matrix `iris_x` and set the number of cluster as `centers = 3` and save the output in a variable called `iris_km`. Note: Call `?kmeans` for help on using the function.
```{r}
iris_km <- kmeans(iris_x, centers = 3)
```

The resulting cluster assignments for each flower can be obtained by extracting the `cluster` entry from the list `iris_km` generated by the `kmeans` function. What does `as.factor` do here?
```{r}
iris_clusters_km <- as.factor(iris_km$cluster)
```

6. Let's compare the clusters we obtained to the labels. For this we can use the `table` command with two arguments: the original species labels `iris_species` and the clusters from our kmeans approach `iris_clusters_km` obtained in the last line of code. How do you interpret the output? (Note: This matrix is also referred to as "confusion matrix").
```{r}
# to be filled
```
7. We can also visualize the cluster labels on our scatterplots. For this generate a scatterplot of Sepal.Length and Sepal.Width coloured by the identified clusters and by the species label and compare the two. You can also use the `shape` and `color` aesthetics in ggplot to show cluster and species in one plot.
```{r}
# to be filled
```

8. Re-run the clustering using the same steps as above with different random seed using the `set.seed()` function before calling the `kmeans` function. What do you find? How does the confusion matrix and the scatterplot change?
```{r}
# to be filled
```

9. Now repeat the clustering with 10 clusters and for three different random seeds. What do you notice?
```{r}
# to be filled
```

Note on exercise 9: You can also set the `nstart` argument in `kmeans` to let the algorithm try different random centers and use the best for the final clustering.

10. Advanced: To determine a good number of clusters we can calculate the within-cluster sum of squares for clustering with different number of clusters. Use a for-loop or the `sapply` function to cluster the data into 1 to 12 clusters and for each clustering extract the within-cluster sum of squares from the output, which you can find in the `tot.withinss` component. Generate a Elbow plot visualising the within-cluster sum of squares for each number of clusters.
```{r}
# to be filled
```

### Hierarchical clustering
11.To use hierarchical clustering we can use the `hclust` function. This function operates on distances objects. Use the distance object you calculated above (or re-calculate the distances between all flowers using `dist`) and call the `hclust` function on it. What class is the output?
```{r}
# to be filled
```

The class has its own plot function. Call the `plot` function on the output of the `hclust` function.
```{r}
# to be filled
```

To generate cluster labels for each flower into 3 clusters we need to cut the tree into three parts. To visualize this, we can plot some boxes that annotate the clusters using the `rect.hclust()` function on the output of the `hclust` function. Let's do this using 3 clusters (`k=3`). Note this required to be directly called together with the `plot` function above because it adds a layer to the existing plot. You can also add the Species names as labels using the `label` argument.
```{r}
plot(hclust.out, labels = FALSE, main = "Iris dendrogram", )
rect.hclust(hclust.out, k= 3, border = 3:5)
```

To extract the cluster labels for each flower we can use
```{r}
clusterid_hc <- cutree(hclust.out, 3)
```


We can again visualize the cluster labels on our scatterplots. Generate a scatterplot of Sepal.Length and Sepal.Width coloured by the identified clusters and by the species label and compare the two. You can also use the `shape` and `color` aesthetics in ggplot to show cluster and species in one plot.
```{r}
# to be filled
```

Exercise: Repeat the above steps to generate two clusters and visualize them in a dendrogram and a scatterplot.
```{r}
# to be filled
```

Question: Do you expect different results from hierarchical clustering for different random seeds if you want to cluster the data into 10 cluster? Test this!

Advanced exercise: Can you also generate a clustering of the 4 features (instead of the samples) using hierarchical clustering? Which features are most similar? What happens if you first use `scale` on the iris_x object? You can also change the distance metric and use for example correlation distance, defined as 1 - correlation. Hint: Use `as.dist()` to transform the results of 1- corrleation into a distance object that `hclust` can handle. How does this change the results?
```{r}
# to be filled
```

The clustering of the `pheatmap` function is also based on hierarchical clustering to order the rows and columns. 
Call this on the Iris data. You can also test different distance measures by specifying the `clustering_distance_cols` or `clustering_distance_rows`. Note: Don't forget to load the pheatmap library to be able to use the function `pheatmap`.
```{r}
# to be filled
```


```{r}
sessionInfo()
```

